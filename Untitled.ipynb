{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interpreted-affiliate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-5e9c70d1e4a7>:306: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  sum_infections = trimed_last_2_wks.groupby('state')['est_inf','new_cases_jhu'].sum().astype('int32')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Latest date of entry in JHU data 2021-02-20\n",
      "\n",
      "Estimated total percent of the US population that\n",
      "has been infected by COVID-19 as of 2021-02-06:      26.8%\n",
      "\n",
      "Reported total percent of the US population that\n",
      "has been infected by COVID-19 as of 2021-02-06:      8.2%\n",
      "\n",
      "Propotion of population in all states that have been infected as of 2021-02-06 (reverse rank ordered)\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "                   state  2019_pop  sum_est_inf  sum_new_cases_jhu  \\\n",
      "30            New Jersey   8882190      3922553             717835   \n",
      "32              New York  19453561      8190490            1470301   \n",
      "24           Mississippi   2976149      1171746             280778   \n",
      "2                Arizona   7278717      2785419             779093   \n",
      "0                Alabama   4903185      1746302             471311   \n",
      "18             Louisiana   4648794      1653212             409861   \n",
      "21         Massachusetts   6892503      2404093             540827   \n",
      "41          South Dakota    884659       307699             109132   \n",
      "6            Connecticut   3565287      1233579             259372   \n",
      "39          Rhode Island   1059361       356526             117891   \n",
      "22              Michigan   9986857      3198874             620685   \n",
      "14               Indiana   6732219      2119169             637987   \n",
      "13              Illinois  12671821      3986669            1144281   \n",
      "38          Pennsylvania  12801989      3946377             869222   \n",
      "3               Arkansas   3017804       928286             306064   \n",
      "40        South Carolina   5148714      1573949             462981   \n",
      "34          North Dakota    762062       230640              98106   \n",
      "42             Tennessee   6829174      2024681             742213   \n",
      "31            New Mexico   2096829       617917             177214   \n",
      "10               Georgia  10617423      3029761             937402   \n",
      "28                Nevada   3080156       871297             283391   \n",
      "15                  Iowa   3155070       892038             324306   \n",
      "8   District of Columbia    705749       191453              37877   \n",
      "16                Kansas   2913314       782918             283926   \n",
      "35                  Ohio  11689100      3009095             918079   \n",
      "43                 Texas  28995881      7401157            2486505   \n",
      "9                Florida  21477737      5376078            1771359   \n",
      "25              Missouri   6137428      1534385             481183   \n",
      "7               Delaware    973764       242990              79832   \n",
      "4             California  39512223      9136006            3408241   \n",
      "26               Montana   1068778       241747              95717   \n",
      "20              Maryland   6045680      1364687             362084   \n",
      "48         West Virginia   1792147       385644             124190   \n",
      "49             Wisconsin   5822434      1185859             600016   \n",
      "17              Kentucky   4467673       899610             376253   \n",
      "36              Oklahoma   3956971       755050             401780   \n",
      "5               Colorado   5758736      1076962             404256   \n",
      "27              Nebraska   1934408       361595             193421   \n",
      "23             Minnesota   5639632      1021084             467217   \n",
      "33        North Carolina  10488084      1875494             791521   \n",
      "12                 Idaho   1787065       317040             165209   \n",
      "50               Wyoming    578759       101458              52618   \n",
      "46              Virginia   8535519      1352402             526176   \n",
      "29         New Hampshire   1359711       200271              68260   \n",
      "37                Oregon   4217737       533577             146741   \n",
      "47            Washington   7614893       829977             320146   \n",
      "44                  Utah   3205958       339758             353700   \n",
      "19                 Maine   1344212       102190              41065   \n",
      "1                 Alaska    731545        47414              55259   \n",
      "45               Vermont    623989        37859              12766   \n",
      "11                Hawaii   1415872        67486              26611   \n",
      "\n",
      "    est_proportion_infected  rep_proportion_infected state_id  \n",
      "30                 0.441620                 0.080817       NJ  \n",
      "32                 0.421028                 0.075580       NY  \n",
      "24                 0.393712                 0.094343       MS  \n",
      "2                  0.382680                 0.107037       AZ  \n",
      "0                  0.356157                 0.096123       AL  \n",
      "18                 0.355622                 0.088165       LA  \n",
      "21                 0.348798                 0.078466       MA  \n",
      "41                 0.347817                 0.123361       SD  \n",
      "6                  0.345997                 0.072749       CT  \n",
      "39                 0.336548                 0.111285       RI  \n",
      "22                 0.320308                 0.062150       MI  \n",
      "14                 0.314780                 0.094766       IN  \n",
      "13                 0.314609                 0.090301       IL  \n",
      "38                 0.308263                 0.067897       PA  \n",
      "3                  0.307603                 0.101419       AR  \n",
      "40                 0.305698                 0.089922       SC  \n",
      "34                 0.302653                 0.128738       ND  \n",
      "42                 0.296475                 0.108683       TN  \n",
      "31                 0.294691                 0.084515       NM  \n",
      "10                 0.285357                 0.088289       GA  \n",
      "28                 0.282874                 0.092005       NV  \n",
      "15                 0.282732                 0.102789       IA  \n",
      "8                  0.271276                 0.053669       DC  \n",
      "16                 0.268738                 0.097458       KS  \n",
      "35                 0.257427                 0.078541       OH  \n",
      "43                 0.255249                 0.085754       TX  \n",
      "9                  0.250309                 0.082474       FL  \n",
      "25                 0.250005                 0.078401       MO  \n",
      "7                  0.249537                 0.081983       DE  \n",
      "4                  0.231220                 0.086258       CA  \n",
      "26                 0.226190                 0.089557       MT  \n",
      "20                 0.225729                 0.059891       MD  \n",
      "48                 0.215185                 0.069297       WV  \n",
      "49                 0.203671                 0.103052       WI  \n",
      "17                 0.201360                 0.084217       KY  \n",
      "36                 0.190815                 0.101537       OK  \n",
      "5                  0.187014                 0.070199       CO  \n",
      "27                 0.186928                 0.099990       NE  \n",
      "23                 0.181055                 0.082845       MN  \n",
      "33                 0.178821                 0.075469       NC  \n",
      "12                 0.177408                 0.092447       ID  \n",
      "50                 0.175303                 0.090915       WY  \n",
      "46                 0.158444                 0.061645       VA  \n",
      "29                 0.147289                 0.050202       NH  \n",
      "37                 0.126508                 0.034791       OR  \n",
      "47                 0.108994                 0.042042       WA  \n",
      "44                 0.105977                 0.110326       UT  \n",
      "19                 0.076022                 0.030549       ME  \n",
      "1                  0.064814                 0.075537       AK  \n",
      "45                 0.060673                 0.020459       VT  \n",
      "11                 0.047664                 0.018795       HI  \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robbdunlap/opt/anaconda3/envs/standard_38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3155: DtypeWarning: Columns (4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from math import isnan, log\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "###### remove extraneous data from the CDC Excess Deaths data set and add a column that is the avg of \"Excess higher Estimate\" and \"Excess Higher Estimate\" ######\n",
    "\n",
    "# Import CDC excess deaths data from file\n",
    "xs_deaths = pd.read_csv(\"data/xs_deaths.csv\")\n",
    "\n",
    "# The CDC data has three entries for each state for each week. These represent whether they are Type \"Predicted(weighted)\" or \"Unweighted\" and Outcome \"All causes\"\n",
    "# or \"All causes, excluding COVID-19\". For this analysis I onlly want the \"Predicte(weighted)\" and \"All causes, excluding COVID-19\". \n",
    "# Droping Type \"All causes\" solves that problem\n",
    "xs_deaths.drop(xs_deaths[(xs_deaths['Outcome'] == 'All causes')].index , inplace=True)\n",
    "\n",
    "# change the \"date\" to datetime type\n",
    "xs_deaths['Week Ending Date'] =  pd.to_datetime(xs_deaths['Week Ending Date'])\n",
    "\n",
    "#capture the lastest date of the CDC data - used later for getting avg of most recent excess deaths\n",
    "latest_date_of_cdc_data = xs_deaths['Week Ending Date'].max()\n",
    "\n",
    "# side quest - save latest date of CDC Excess Deaths data to file for use by the display module\n",
    "date_of_reporting = str(latest_date_of_cdc_data)[:10]\n",
    "file_path = 'data/latest_date_of_excess_deaths_data.txt'\n",
    "with open(file_path, 'w+') as filetowrite:\n",
    "    filetowrite.write(date_of_reporting)\n",
    "\n",
    "# Drop PR and the \"all US\" data\n",
    "indexNames = xs_deaths[ (xs_deaths['State'] == 'Puerto Rico') | \n",
    "                        (xs_deaths['State'] == 'United States')].index\n",
    "\n",
    "xs_deaths.drop(indexNames , inplace=True) \n",
    "\n",
    "# Select only values that match the time period for the JHU data\n",
    "xs_deaths_2020 = xs_deaths[(xs_deaths['Week Ending Date'] > '2020-01-21')].copy()\n",
    "del xs_deaths\n",
    "\n",
    "# drop the unnecessary columns \n",
    "xs_deaths_2020 = xs_deaths_2020.drop(xs_deaths_2020.columns[[2, 3, 4, 5, 8, 9, 10, 11, 12, 13, 14, 15, 16]], axis=1)\n",
    "\n",
    "# add the average of the upper and lower xs deaths to the df\n",
    "xs_deaths_2020['mid_point_xs_deaths'] = (xs_deaths_2020['Excess Lower Estimate'] + xs_deaths_2020['Excess Higher Estimate'])/2\n",
    "\n",
    "# rename columns\n",
    "xs_deaths_2020.rename(columns={\"Week Ending Date\": \"date\", \"State\":\"state\"}, inplace=True)\n",
    "\n",
    "## merge the NYC city with the NY state data (CDC had the data broken out separately) ##\n",
    "# merge the data\n",
    "ny_data = xs_deaths_2020.loc[xs_deaths_2020['state'].str.contains('New York')].groupby('date').sum()\n",
    "\n",
    "# Add a new 'state' column for merging back with the xs_deaths_2020 df\n",
    "ny_data['state'] = 'New York'\n",
    "ny_data.reset_index(inplace=True)\n",
    "\n",
    "# change the order of the columns (probably delete this later)\n",
    "ny_data = ny_data[['date', 'state', 'Excess Lower Estimate', 'Excess Higher Estimate', 'mid_point_xs_deaths']]\n",
    "\n",
    "# drop the NY and NYC data from the xs_deaths_2020 df\n",
    "xs_deaths_2020.drop(xs_deaths_2020[xs_deaths_2020['state'].str.contains('New York')].index , inplace=True)\n",
    "\n",
    "# concatenate xs_death_2020 and ny_data so the df has all NY (NYC and NY state) data as NY state\n",
    "dfs_to_concat = [xs_deaths_2020, ny_data]\n",
    "xs_deaths_2020_ny = pd.concat(dfs_to_concat)\n",
    "del xs_deaths_2020\n",
    "del ny_data\n",
    "\n",
    "# sort the data to be tidy :-)\n",
    "xs_deaths_2020_ny.sort_values(by=['state','date'], inplace=True)\n",
    "xs_deaths_2020_ny.reset_index(inplace=True)\n",
    "del xs_deaths_2020_ny['index']\n",
    "\n",
    "###### end/remove extraneous data from the CDC Excess Deaths data set ######\n",
    "\n",
    "\n",
    "\n",
    "###### munge JHU data and remove extraneous data ######\n",
    "\n",
    "# import data files\n",
    "new_cases = pd.read_csv(\"data/jhu_confirmed_daily.csv\")\n",
    "new_deaths = pd.read_csv(\"data/jhu_deaths_daily.csv\")\n",
    "\n",
    "# drop the following columns because they aren't important to this analysis\n",
    "new_cases.drop([\"UID\", \"iso2\", \"iso3\", \"code3\", \"FIPS\", \"Country_Region\", \"Lat\", \"Long_\", \"Combined_Key\"], axis=1, inplace=True)\n",
    "new_deaths.drop([\"UID\", \"iso2\", \"iso3\", \"code3\", \"FIPS\", \"Country_Region\", \"Lat\", \"Long_\", \"Combined_Key\", \"Population\"], axis=1, inplace=True)\n",
    "\n",
    "# melt into a long form instead of wide\n",
    "new_cases_long = new_cases.melt(id_vars=[\"Admin2\", \"Province_State\"], var_name=\"Date\", value_name=\"New_Cases\")\n",
    "new_deaths_long = new_deaths.melt(id_vars=[\"Admin2\", \"Province_State\"], var_name=\"Date\", value_name=\"Deaths\")\n",
    "del new_cases\n",
    "del new_deaths\n",
    "\n",
    "# convert the date column from a string into datetime\n",
    "new_cases_long['Date'] = pd.to_datetime(new_cases_long['Date'])\n",
    "new_deaths_long['Date'] = pd.to_datetime(new_deaths_long['Date'])\n",
    "\n",
    "## JHU data is at the county level - sum all the data from counties into state level ##\n",
    "# groupby/sum - from https://jamesrledoux.com/code/group-by-aggregate-pandas\n",
    "# output -> index, Province_State, Date, new_cases\n",
    "new_cases_by_state = new_cases_long.groupby(['Province_State','Date']).agg({'New_Cases':['sum']})\n",
    "del new_cases_long\n",
    "new_cases_by_state.columns = ['new_cases']\n",
    "new_cases_by_state = new_cases_by_state.reset_index()\n",
    "new_cases_by_state.sort_values(['Province_State','Date'], inplace=True)\n",
    "\n",
    "# output -> index, Province_State, Date, new_deaths\n",
    "new_deaths_by_state = new_deaths_long.groupby(['Province_State','Date']).agg({'Deaths':['sum']})\n",
    "del new_deaths_long\n",
    "new_deaths_by_state.columns = ['new_deaths']\n",
    "new_deaths_by_state = new_deaths_by_state.reset_index()\n",
    "new_deaths_by_state.sort_values(['Province_State','Date'], inplace=True)\n",
    "\n",
    "# replacing the administrative corrections that cause an artificial bump in NJ's deaths on 6/25/20\n",
    "# https://kywnewsradio.radio.com/articles/news/new-jersey-now-reporting-probable-covid-19-deaths\n",
    "\n",
    "new_deaths_by_state_fixed = new_deaths_by_state.copy()\n",
    "del new_deaths_by_state\n",
    "\n",
    "deaths_NJ_6_24 = new_deaths_by_state_fixed['new_deaths'].loc[(new_deaths_by_state_fixed['Province_State'] == 'New Jersey') & \n",
    "                                      (new_deaths_by_state_fixed['Date'] == '2020-06-24')].values[0]\n",
    "\n",
    "deaths_NJ_6_25 = new_deaths_by_state_fixed['new_deaths'].loc[(new_deaths_by_state_fixed['Province_State'] == 'New Jersey') & \n",
    "                                      (new_deaths_by_state_fixed['Date'] == '2020-06-25')].values[0]\n",
    "\n",
    "deaths_NJ_6_26 = new_deaths_by_state_fixed['new_deaths'].loc[(new_deaths_by_state_fixed['Province_State'] == 'New Jersey') & \n",
    "                                      (new_deaths_by_state_fixed['Date'] == '2020-06-26')].values[0]\n",
    "\n",
    "substitution_value = ((deaths_NJ_6_24+deaths_NJ_6_26)/2)\n",
    "\n",
    "new_deaths_by_state_fixed.loc[(new_deaths_by_state_fixed['Province_State'] == 'New Jersey') & \n",
    "                                      (new_deaths_by_state_fixed['Date'] == '2020-06-25'), 'new_deaths'] = substitution_value\n",
    "\n",
    "del deaths_NJ_6_26\n",
    "\n",
    "# Outer Join of the daily_cases and daily_deaths tables\n",
    "combined_daily_cases_deaths = pd.merge(new_cases_by_state,new_deaths_by_state_fixed,on=['Province_State','Date'],how='outer')\n",
    "del new_cases_by_state\n",
    "del new_deaths_by_state_fixed\n",
    "\n",
    "# drop Diamond Princess, Guam, American Somoa, Northern Mariana Islands, and Puerto Rico (other data sets don't have the territories)\n",
    "combined_daily_cases_deaths.drop(combined_daily_cases_deaths.loc[combined_daily_cases_deaths['Province_State'] == \"Diamond Princess\"].index, inplace=True)\n",
    "combined_daily_cases_deaths.drop(combined_daily_cases_deaths.loc[combined_daily_cases_deaths['Province_State'] == \"Grand Princess\"].index, inplace=True)\n",
    "combined_daily_cases_deaths.drop(combined_daily_cases_deaths.loc[combined_daily_cases_deaths['Province_State'] == \"American Samoa\"].index, inplace=True)\n",
    "combined_daily_cases_deaths.drop(combined_daily_cases_deaths.loc[combined_daily_cases_deaths['Province_State'] == \"Guam\"].index, inplace=True)\n",
    "combined_daily_cases_deaths.drop(combined_daily_cases_deaths.loc[combined_daily_cases_deaths['Province_State'] == \"Northern Mariana Islands\"].index, inplace=True)\n",
    "combined_daily_cases_deaths.drop(combined_daily_cases_deaths.loc[combined_daily_cases_deaths['Province_State'] == \"Puerto Rico\"].index, inplace=True)\n",
    "combined_daily_cases_deaths.drop(combined_daily_cases_deaths.loc[combined_daily_cases_deaths['Province_State'] == \"Virgin Islands\"].index, inplace=True)\n",
    "\n",
    "# save the processed data as a csv (for use by other applications such as the Streamlit webapp)\n",
    "combined_daily_cases_deaths.to_csv(\"data/daily_cases_deaths.csv\", index=False)\n",
    "\n",
    "###### end/munge JHU data, remove extraneous data, and add 7-day rolling averages ######\n",
    "\n",
    "\n",
    "\n",
    "###### create a table of the JHU data summed into weekly values instead of daily and merge with CDC excess deaths data ######\n",
    "\n",
    "daily_cases_deaths = combined_daily_cases_deaths\n",
    "del combined_daily_cases_deaths\n",
    "\n",
    "# rename columns because I prefer this convention\n",
    "daily_cases_deaths.rename(columns={\"Province_State\": \"state\", \"Date\": \"date\", \"new_cases\": \"new_cases_jhu\", \"new_deaths\":\"new_deaths_jhu\"}, inplace=True)\n",
    "\n",
    "# sum the weekly cases, match the week starting date to the one used by the CDC\n",
    "weekly_cases_deaths = daily_cases_deaths.groupby('state').resample('W-SAT', on='date').sum()\n",
    "weekly_cases_deaths['new_deaths_jhu'] = weekly_cases_deaths['new_deaths_jhu'].astype(int)\n",
    "\n",
    "# delete the lastest week if it's not a complete week\n",
    "latest_date_of_jhu_data = daily_cases_deaths['date'].max()\n",
    "\n",
    "# side quest - save latest date of JHU data to file for use by the display module\n",
    "date_of_reporting = str(latest_date_of_jhu_data)[:10]\n",
    "file_path = 'data/latest_date_of_JHU_data.txt'\n",
    "with open(file_path, 'w+') as filetowrite:\n",
    "    filetowrite.write(date_of_reporting)\n",
    "\n",
    "# reset the index so 'date' is a column\n",
    "weekly_cases_deaths.reset_index(inplace=True)\n",
    "\n",
    "# remove the latest week's data unless the download data is through Saturday of that week (Sat is a complete week)\n",
    "# any other day of the week means partial weeks data\n",
    "if latest_date_of_jhu_data.weekday() != 5:\n",
    "    latest_date_of_weekly_cases_deaths = weekly_cases_deaths['date'].max()\n",
    "    weekly_cases_deaths = weekly_cases_deaths[weekly_cases_deaths.date != latest_date_of_weekly_cases_deaths]\n",
    "\n",
    "# free memory\n",
    "del daily_cases_deaths\n",
    "\n",
    "# merge the CDC weekly excess deaths with the JMU deaths data\n",
    "weekly_cases_deaths_xs = pd.merge(weekly_cases_deaths, xs_deaths_2020_ny, how='left', left_on=['state','date'], right_on=['state','date']) \n",
    "\n",
    "###### end/group the JHU data into weekly values instead of daily ######\n",
    "\n",
    "###### get the average of the last 4 values mid_point_xs_deaths\n",
    "\n",
    "select_back_to = latest_date_of_cdc_data - timedelta(weeks=4) # this gives the date of the 5th week back\n",
    "\n",
    "# select only the latest 4 weeks of data from the CDC excess deaths data\n",
    "latest_weeks_xs_deaths = weekly_cases_deaths_xs[(weekly_cases_deaths_xs['date'] > select_back_to) & \n",
    "                                                (weekly_cases_deaths_xs['date'] <= latest_date_of_cdc_data)].copy()\n",
    "\n",
    "# average the latest 4 weeks data\n",
    "latest_weeks_xs_deaths_avg = latest_weeks_xs_deaths.groupby('state').mean()\n",
    "\n",
    "# some states (NC) are not reporting data in recent weeks\n",
    "latest_weeks_xs_deaths_avg = latest_weeks_xs_deaths_avg.fillna(0)\n",
    "\n",
    "# convert the results to integers (rounds down)\n",
    "latest_weeks_xs_deaths_avg = latest_weeks_xs_deaths_avg.astype(int)\n",
    "\n",
    "# add the dates to indicate what date range the averages were taken from\n",
    "latest_weeks_xs_deaths_avg['data_from_dates_after'] = select_back_to\n",
    "latest_weeks_xs_deaths_avg['data_from_dates_up_to'] = latest_date_of_cdc_data\n",
    "\n",
    "# reset the index\n",
    "latest_weeks_xs_deaths_avg.reset_index(inplace=True)\n",
    "\n",
    "###### end/get the average of the last 4 values mid_point_xs_deaths ######\n",
    "\n",
    "\n",
    "\n",
    "###### calculating estimated actual deaths per day from CDC xs deaths and JHU reported deaths data ######\n",
    "\n",
    "# create a dictionary of the mid_point_xs_deaths to make calculation quicker in the df\n",
    "state_xs_deaths_dict = latest_weeks_xs_deaths_avg.set_index('state').to_dict()['mid_point_xs_deaths']\n",
    "\n",
    "# the new-improved version that doesn't estimate excess deaths for when a state has less than\n",
    "# 6 deaths that week. Instead, it just uses the JHU reported deaths value and doesn't make a\n",
    "# correction.\n",
    "def est_deaths(row):\n",
    "    if row['new_deaths_jhu'] < 6:\n",
    "        corrected_value = row.fillna(0)['new_deaths_jhu']\n",
    "    elif row['date'] <= latest_date_of_cdc_data:\n",
    "        corrected_value = row.fillna(0)['new_deaths_jhu'] + row.fillna(0)['mid_point_xs_deaths']\n",
    "    else:\n",
    "        corrected_value = row.fillna(0)['new_deaths_jhu'] + state_xs_deaths_dict[row['state']]\n",
    "    return corrected_value\n",
    "\n",
    "# apply the calculation to each row\n",
    "weekly_cases_deaths_xs['corr_new_deaths'] = weekly_cases_deaths_xs.apply(est_deaths, axis=1)\n",
    "\n",
    "# save the data as a csv for the other modules\n",
    "weekly_cases_deaths_xs.to_csv(\"data/weekly_cases_deaths_xs.csv\", index=False)\n",
    "\n",
    "###### end/calculating estimated actual deaths per day from CDC xs deaths and JHU reported deaths data ######\n",
    "\n",
    "\n",
    "\n",
    "###### add the estimated infections that occured 2 weeks previous to the date of the corrected deaths ######\n",
    "\n",
    "est_infections = weekly_cases_deaths_xs[['state', 'date','corr_new_deaths']].copy()\n",
    "\n",
    "est_infections['offset_date'] = est_infections['date'] - timedelta(weeks=2)\n",
    "\n",
    "# this is the inverse of the estimated infection fatality ratio - taken from literature\n",
    "# https://www.medrxiv.org/content/10.1101/2020.05.03.20089854v4\n",
    "# https://www.cdc.gov/coronavirus/2019-ncov/hcp/planning-scenarios.html\n",
    "estimated_ifr = 1/0.0068\n",
    "\n",
    "est_infections['est_inf'] = est_infections['corr_new_deaths'] * estimated_ifr\n",
    "est_infections['est_inf'] = est_infections['est_inf'].astype(int)\n",
    "\n",
    "\n",
    "# drop estimated infections before the start of the JHU dataset - not critical, just because\n",
    "est_infections.drop(est_infections[est_infections['offset_date'] < '2020-01-22'].index , inplace=True)\n",
    "\n",
    "# drop rows that were used to generate data but are not needed in the merge\n",
    "est_infections.drop(['date', 'corr_new_deaths'], axis = 1, inplace=True) \n",
    "\n",
    "# merge the estimate infections back with weekly_cases_deaths_xs. This will result\n",
    "# in nan values for the lastest two weeks. This method back-calculates the estimated \n",
    "# number of infections that had to have occured to cause the specified number of deaths,\n",
    "# therefore the latest two weeks will have nan values. These values will be filled in \n",
    "# as the pandemic progresses.\n",
    "weekly_est_cases_deaths = weekly_cases_deaths_xs.merge(est_infections, \n",
    "                                                       left_on=['state','date'], \n",
    "                                                       right_on=['state','offset_date'],\n",
    "                                                       how='outer')\n",
    "\n",
    "###### end/add the estimated infections that occured 2 weeks previous to the date of the corrected deaths ######\n",
    "\n",
    "\n",
    "\n",
    "###### add percent change in weekly est_inf ######\n",
    "\n",
    "def per_change_est_cases_func(rows):\n",
    "    # temporarily set panda options so that values with \"divide by 0\" are converted to nan instead of infinity\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        # this function calculates the proportion, not the percentage, hence the variable name 'pro_chag_est_inf'\n",
    "        rows['pro_chg_est_inf'] = (rows['est_inf'] - rows['est_inf'].shift(1)) / rows['est_inf'].shift(1)    \n",
    "        return rows\n",
    "\n",
    "weekly_est_cases_deaths = weekly_est_cases_deaths.groupby('state').apply(per_change_est_cases_func)\n",
    "weekly_est_cases_deaths['pro_chg_est_inf'] = weekly_est_cases_deaths['pro_chg_est_inf'].round(4)\n",
    "\n",
    "###### end/add percent change in weekly est_inf ######\n",
    "\n",
    "\n",
    "###### calculating the estimated proportion of the population that has already been infected with COVID-19 ######\n",
    "\n",
    "# remove the last 2 weeks of data for each state because we don't have estimated infections for those dates\n",
    "max_date_est_inf = weekly_est_cases_deaths['offset_date'].max()\n",
    "trimed_last_2_wks = weekly_est_cases_deaths[(weekly_est_cases_deaths['date'] <= max_date_est_inf)]\n",
    "\n",
    "sum_infections = trimed_last_2_wks.groupby('state')['est_inf','new_cases_jhu'].sum().astype('int32')\n",
    "\n",
    "# add state population data\n",
    "state_pop = pd.read_csv('data/state_pop.csv')\n",
    "\n",
    "percent_pop_infected = state_pop.merge(sum_infections, left_on=\"NAME\", right_on=\"state\")\n",
    "percent_pop_infected['est_proportion_infected'] = (percent_pop_infected['est_inf'] /\n",
    "                                              percent_pop_infected['POPESTIMATE2019'])\n",
    "percent_pop_infected['rep_proportion_infected'] = (percent_pop_infected['new_cases_jhu'] /\n",
    "                                              percent_pop_infected['POPESTIMATE2019'])\n",
    "\n",
    "percent_pop_infected.sort_values(by=['est_proportion_infected'], ascending=False, inplace=True)\n",
    "percent_pop_infected.rename(columns={\"NAME\": \"state\", \"POPESTIMATE2019\": \"2019_pop\", \"est_inf\":\"sum_est_inf\", \"new_cases_jhu\":\"sum_new_cases_jhu\"}, inplace=True)\n",
    "\n",
    "total_pop = percent_pop_infected['2019_pop'].sum()\n",
    "total_infected = percent_pop_infected['sum_est_inf'].sum()\n",
    "total_reported_infected = percent_pop_infected['sum_new_cases_jhu'].sum()\n",
    "\n",
    "percent_total_us_pop_est_infected = f\"{100*total_infected/total_pop:.1f}\"\n",
    "percent_total_us_pop_reported_infected = f\"{100*total_reported_infected/total_pop:.1f}\"\n",
    "\n",
    "# side quest - save the estimated and reported percent of total US pop infected to date to file for use by the display module\n",
    "file_path = 'data/estimated_percent_US_infected.txt'\n",
    "with open(file_path, 'w+') as filetowrite:\n",
    "    filetowrite.write(percent_total_us_pop_est_infected)\n",
    "\n",
    "file_path = 'data/reported_percent_US_infected.txt'\n",
    "with open(file_path, 'w+') as filetowrite:\n",
    "    filetowrite.write(percent_total_us_pop_reported_infected)\n",
    "\n",
    "# from https://worldpopulationreview.com/states/state-abbreviations\n",
    "state_abbrevs = pd.read_csv('data/state_abbrevs.csv')\n",
    "\n",
    "def add_state_id(row):\n",
    "    state = row['state']\n",
    "    state_id = state_abbrevs[state_abbrevs['State'] == state]['Code']\n",
    "    return state_id.values[0]\n",
    "    \n",
    "percent_pop_infected['state_id'] =  percent_pop_infected.apply(add_state_id, axis=1)\n",
    "\n",
    "latest_date_of_estimate = str(max_date_est_inf)[:10]\n",
    "date_of_reporting = str(latest_date_of_jhu_data)[:10]\n",
    "\n",
    "# drop the offset date column because it's no longer necessary\n",
    "del weekly_est_cases_deaths['offset_date']\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(f'Latest date of entry in JHU data {date_of_reporting}\\n')\n",
    "print('Estimated total percent of the US population that')\n",
    "print(f'has been infected by COVID-19 as of {latest_date_of_estimate}:      '+str(percent_total_us_pop_est_infected)+'%\\n')\n",
    "print('Reported total percent of the US population that')\n",
    "print(f'has been infected by COVID-19 as of {latest_date_of_estimate}:      '+str(percent_total_us_pop_reported_infected)+'%\\n')\n",
    "print(f'Propotion of population in all states that have been infected as of {latest_date_of_estimate} (reverse rank ordered)')\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "print(percent_pop_infected)\n",
    "print()\n",
    "print()\n",
    "\n",
    "# save proportion of population infected to date for the display module\n",
    "percent_pop_infected.drop(['sum_est_inf','sum_new_cases_jhu','state_id'], axis=1, inplace=True)\n",
    "percent_pop_infected.to_csv('data/proportion_pop_infected.csv',index=False)\n",
    "\n",
    "###### end/calculating the estimated proportion of the population that has already been infected with COVID-19 ######\n",
    "\n",
    "\n",
    "\n",
    "###### estimating the population size of people infectious at specific dates ######\n",
    "# in the original version of this analysis I was calculating total number of infections\n",
    "# on a daily basis. In this version I switched to calculating infections on a weekly \n",
    "# basis to improve the statistical quality of the data. Previously I was calculating the \n",
    "# number of people who were in an infectious state by summing all the people infected \n",
    "# between 4 and 11 days prior to the day of interest. That won't work with this method\n",
    "# since I'm summing up infections by week. I could still maintain a count of infections by \n",
    "# day but I don't think it would be that much of a change relative to just using the count\n",
    "# of the people infected in the previous week as the number of people in an infectious \n",
    "# state - I think it's a reasonable approximation for a machine learning approach.\n",
    "\n",
    "\n",
    "# formula to sum up all the people who were infected between 4-10 days previous to the \n",
    "# current date being evaluated (literature estimate of avg infectious period is 7 days)\n",
    "def infectious_count(row):\n",
    "    offset_index = row.name - 1\n",
    "    if row['state'] == weekly_est_cases_deaths.iloc[offset_index,0]:\n",
    "        if isnan(weekly_est_cases_deaths.iloc[offset_index,9]):\n",
    "            pass\n",
    "        else:\n",
    "            infectious_populace = weekly_est_cases_deaths.iloc[offset_index,8]\n",
    "            return infectious_populace\n",
    "\n",
    "weekly_est_cases_deaths[\"mobile_infectious\"] = weekly_est_cases_deaths.apply(infectious_count, axis=1)\n",
    "\n",
    "def proportion_infected(row):\n",
    "    state = row['state']\n",
    "    pop = percent_pop_infected[percent_pop_infected['state'] == state]['2019_pop']\n",
    "    phi = row['mobile_infectious'] / pop.values[0]\n",
    "    return phi\n",
    "\n",
    "def population(row):\n",
    "    state = row['state']\n",
    "    pop = percent_pop_infected[percent_pop_infected['state'] == state]['2019_pop']\n",
    "    return pop.values[0]\n",
    "\n",
    "# these functions take a while to run because of the recursive iteration\n",
    "weekly_est_cases_deaths[\"phi\"] = weekly_est_cases_deaths.apply(proportion_infected, axis=1)\n",
    "weekly_est_cases_deaths[\"population\"] = weekly_est_cases_deaths.apply(population, axis=1)\n",
    "\n",
    "###### end/estimating the population size of people infectious at specific dates ######\n",
    "\n",
    "\n",
    "###### estimating population density in states ######\n",
    "\n",
    "# State pop divided by state size is not sufficient. An excellent example of this Alaska. Greater than \n",
    "# 90% of the population lives in 10 towns. The total area these people occupy is probably way less than 10% \n",
    "# of the size of the state. The actual average population density people exerience is therefor way higher than the\n",
    "# value you would get by dividing state pop by state size. I coudln't find free published data that calculated \n",
    "# effective density so this section makes an approximation by calculating the proportional density of each counties\n",
    "# (sum of each county's population divided by the county's land area multiplied by the percent state population\n",
    "# that lives in the county). This method probably under-estimates effective density but is better than \n",
    "# state pop/state area.\n",
    "\n",
    "# The Census Bureau is publishing population statistics for COVID data science users:\n",
    "# https://covid19.census.gov/datasets/average-household-size-and-population-density-county/data?geometry=125.999%2C-0.672%2C-125.368%2C76.524\n",
    "\n",
    "# column headings\n",
    "# OBJECTID\t GNIS County Code\t Geographic Identifier - FIPS Code\t Area of Land (square meters)\t Area of Water (square meters)\t Name\t \n",
    "# State\t Average Household Size\t Average Household Size - Margin of Error\t Average Household Size of Owner-Occupied Unit\t \n",
    "# Average Household Size of Owner-Occupied Unit - Margin of Error\t Average Household Size of Renter-Occupied Unit\t \n",
    "# Average Household Size of Renter-Occupied Unit - Margin of Error\t Total Population\t Total Population - Margin of Error\t \n",
    "# Population Density (people per square kilometer)\t created_user\t created_date\t last_edited_user\t last_edited_date\t Shape__Area\t \n",
    "# Shape__Length\t Population Density - Margin of Error\n",
    "\n",
    "fips_density = pd.read_csv('data/Average_Household_Size_and_Population_Density_-_County.csv')\n",
    "\n",
    "state_density = fips_density[['State', 'NAME', 'B01001_001E', 'B01001_calc_PopDensity']].copy()\n",
    "del fips_density\n",
    "state_density.rename(columns={'State':'state','NAME':'county','B01001_001E':'county_pop', 'B01001_calc_PopDensity':'county_pop_density'}, inplace=True)\n",
    "\n",
    "state_pops = state_density.groupby('state')['county_pop'].sum()\n",
    "\n",
    "state_density = state_density.merge(state_pops, left_on='state', right_on='state', how='outer') \n",
    "state_density.rename(columns={'county_pop_x':'county_pop','county_pop_y':'state_pop'}, inplace=True)\n",
    "\n",
    "# pop density is people per square kilometer\n",
    "state_density['proportional_co_density'] = state_density['county_pop'] / state_density['state_pop'] * state_density['county_pop_density']\n",
    "\n",
    "state_densities = state_density.groupby('state')['proportional_co_density'].sum()\n",
    "state_densities = pd.DataFrame(data=state_densities)\n",
    "state_densities.reset_index(inplace=True)\n",
    "state_densities.rename(columns={'proportional_co_density':'state_density'}, inplace=True)\n",
    "\n",
    "# convert densities to natural logs to make the differences between states more linear\n",
    "def log_density(row):\n",
    "    state_density = row['state_density']\n",
    "    log_state_density = log(state_density)\n",
    "    return log_state_density\n",
    "\n",
    "state_densities['log_state_density'] = state_densities.apply(log_density, axis=1)\n",
    "state_densities.sort_values(by=['state_density'], inplace=True)\n",
    "\n",
    "# compress the ln scale of state densities between 0 and 1.\n",
    "# rho is the normalized avg_city_density index between 0 and 1 (1 being set by the most dense state)\n",
    "highest_density = state_densities.nlargest(1, 'log_state_density')\n",
    "state_densities['proportionate_log_density'] = state_densities['log_state_density'] / highest_density.values[0][2]\n",
    "state_densities.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# add est_inf/100,000 population\n",
    "weekly_est_cases_deaths['est_inf_per_100k'] = weekly_est_cases_deaths['est_inf'] / (weekly_est_cases_deaths['population'] / 100000)\n",
    "weekly_est_cases_deaths['new_cases_per_100k'] = weekly_est_cases_deaths['new_cases_jhu'] / (weekly_est_cases_deaths['population'] / 100000)\n",
    "\n",
    "# add the two letter state code to the df for plotting purposes\n",
    "def state_id(row):\n",
    "    state = row['state']\n",
    "    state_id = state_abbrevs[state_abbrevs['State'] == state]['Code']\n",
    "    return state_id.values[0]\n",
    "\n",
    "weekly_est_cases_deaths['state_id'] =  weekly_est_cases_deaths.apply(state_id, axis=1)\n",
    "\n",
    "# add proportionate (b/w 0 and 1) log density for each state\n",
    "def add_rho(row):\n",
    "    state = row['state']\n",
    "    rho = state_densities[state_densities['state'] == state]['proportionate_log_density']\n",
    "    return rho.values[0]\n",
    "\n",
    "weekly_est_cases_deaths['rho'] =  weekly_est_cases_deaths.apply(add_rho, axis=1)\n",
    "\n",
    "###### end/estimating population density in states ######\n",
    "\n",
    "\n",
    "\n",
    "############ create exposure index using google mobility data ###################\n",
    "mobility_data = pd.read_csv(\"data/Global_Mobility_Report.csv\")\n",
    "\n",
    "# drop non-US data\n",
    "mobility_data.drop(mobility_data.loc[mobility_data['country_region_code'] != \"US\"].index, inplace=True)\n",
    "\n",
    "# drop county data, only using state level data for this analysis\n",
    "mobility_data = mobility_data[mobility_data['sub_region_2'].isna()]\n",
    "\n",
    "# drop other unnecessary columns\n",
    "mobility_data.drop(['country_region_code','country_region','iso_3166_2_code','census_fips_code','metro_area','sub_region_2'], axis=1, inplace=True)\n",
    "\n",
    "# convert date column to datetime formate\n",
    "mobility_data['date'] = pd.to_datetime(mobility_data['date'])\n",
    "\n",
    "# there are values that are \"US\" but don't have a sub_region_1 - they're probably all US - drop these\n",
    "mobility_data.dropna(subset=['sub_region_1'], inplace=True)\n",
    "\n",
    "mob_data_for_plotting = mobility_data.iloc[:,0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proud-binary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18768 entries, 3323649 to 4251359\n",
      "Data columns (total 9 columns):\n",
      " #   Column                                              Non-Null Count  Dtype         \n",
      "---  ------                                              --------------  -----         \n",
      " 0   sub_region_1                                        18768 non-null  object        \n",
      " 1   place_id                                            18768 non-null  object        \n",
      " 2   date                                                18768 non-null  datetime64[ns]\n",
      " 3   retail_and_recreation_percent_change_from_baseline  18768 non-null  float64       \n",
      " 4   grocery_and_pharmacy_percent_change_from_baseline   18768 non-null  float64       \n",
      " 5   parks_percent_change_from_baseline                  18311 non-null  float64       \n",
      " 6   transit_stations_percent_change_from_baseline       18633 non-null  float64       \n",
      " 7   workplaces_percent_change_from_baseline             18768 non-null  float64       \n",
      " 8   residential_percent_change_from_baseline            18768 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(6), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "mobility_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "leading-volume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_region_1</th>\n",
       "      <th>place_id</th>\n",
       "      <th>date</th>\n",
       "      <th>retail_pcfb</th>\n",
       "      <th>groc_pcfb</th>\n",
       "      <th>parks_pcfb</th>\n",
       "      <th>transit_pcfb</th>\n",
       "      <th>work_pcfb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3323649</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323650</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323651</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323652</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323653</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sub_region_1                     place_id       date  retail_pcfb  \\\n",
       "3323649      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-15          5.0   \n",
       "3323650      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-16          0.0   \n",
       "3323651      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-17          3.0   \n",
       "3323652      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-18         -4.0   \n",
       "3323653      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-19          4.0   \n",
       "\n",
       "         groc_pcfb  parks_pcfb  transit_pcfb  work_pcfb  \n",
       "3323649        2.0        39.0           7.0        2.0  \n",
       "3323650       -2.0        -7.0           3.0       -1.0  \n",
       "3323651        0.0        17.0           7.0      -17.0  \n",
       "3323652       -3.0       -11.0          -1.0        1.0  \n",
       "3323653        1.0         6.0           4.0        1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mob_data_for_plotting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ranking-subscription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_region_1</th>\n",
       "      <th>place_id</th>\n",
       "      <th>date</th>\n",
       "      <th>retail_and_recreation_percent_change_from_baseline</th>\n",
       "      <th>grocery_and_pharmacy_percent_change_from_baseline</th>\n",
       "      <th>parks_percent_change_from_baseline</th>\n",
       "      <th>transit_stations_percent_change_from_baseline</th>\n",
       "      <th>workplaces_percent_change_from_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3323649</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323650</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323651</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323652</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323653</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sub_region_1                     place_id       date  \\\n",
       "3323649      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-15   \n",
       "3323650      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-16   \n",
       "3323651      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-17   \n",
       "3323652      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-18   \n",
       "3323653      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-19   \n",
       "\n",
       "         retail_and_recreation_percent_change_from_baseline  \\\n",
       "3323649                                                5.0    \n",
       "3323650                                                0.0    \n",
       "3323651                                                3.0    \n",
       "3323652                                               -4.0    \n",
       "3323653                                                4.0    \n",
       "\n",
       "         grocery_and_pharmacy_percent_change_from_baseline  \\\n",
       "3323649                                                2.0   \n",
       "3323650                                               -2.0   \n",
       "3323651                                                0.0   \n",
       "3323652                                               -3.0   \n",
       "3323653                                                1.0   \n",
       "\n",
       "         parks_percent_change_from_baseline  \\\n",
       "3323649                                39.0   \n",
       "3323650                                -7.0   \n",
       "3323651                                17.0   \n",
       "3323652                               -11.0   \n",
       "3323653                                 6.0   \n",
       "\n",
       "         transit_stations_percent_change_from_baseline  \\\n",
       "3323649                                            7.0   \n",
       "3323650                                            3.0   \n",
       "3323651                                            7.0   \n",
       "3323652                                           -1.0   \n",
       "3323653                                            4.0   \n",
       "\n",
       "         workplaces_percent_change_from_baseline  \n",
       "3323649                                      2.0  \n",
       "3323650                                     -1.0  \n",
       "3323651                                    -17.0  \n",
       "3323652                                      1.0  \n",
       "3323653                                      1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mob_data_for_plotting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "golden-advancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_region_1</th>\n",
       "      <th>place_id</th>\n",
       "      <th>date</th>\n",
       "      <th>retail_pcfb</th>\n",
       "      <th>groc_pcfb</th>\n",
       "      <th>parks_pcfb</th>\n",
       "      <th>transit_pcfb</th>\n",
       "      <th>work_pcfb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3323649</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-15</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323650</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323651</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-17</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323652</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-18</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323653</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>ChIJdf5LHzR_hogR6czIUzU0VV4</td>\n",
       "      <td>2020-02-19</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sub_region_1                     place_id       date  retail_pcfb  \\\n",
       "3323649      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-15          5.0   \n",
       "3323650      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-16          0.0   \n",
       "3323651      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-17          3.0   \n",
       "3323652      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-18         -4.0   \n",
       "3323653      Alabama  ChIJdf5LHzR_hogR6czIUzU0VV4 2020-02-19          4.0   \n",
       "\n",
       "         groc_pcfb  parks_pcfb  transit_pcfb  work_pcfb  \n",
       "3323649        2.0        39.0           7.0        2.0  \n",
       "3323650       -2.0        -7.0           3.0       -1.0  \n",
       "3323651        0.0        17.0           7.0      -17.0  \n",
       "3323652       -3.0       -11.0          -1.0        1.0  \n",
       "3323653        1.0         6.0           4.0        1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mob_data_for_plotting.rename(columns={'retail_and_recreation_percent_change_from_baseline':'retail_pcfb',\n",
    "                     'grocery_and_pharmacy_percent_change_from_baseline':'groc_pcfb',\n",
    "                     'parks_percent_change_from_baseline':'parks_pcfb',\n",
    "                     'transit_stations_percent_change_from_baseline':'transit_pcfb',\n",
    "                     'workplaces_percent_change_from_baseline':'work_pcfb',\n",
    "                     'residential_percent_change_from_baseline':'res_pcfb'}, inplace=True)\n",
    "\n",
    "mob_data_for_plotting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "expired-wallet",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['res_pcfb'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1d503c380475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# count the number of nan values in the mobility data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m count_nan_mob_data_for_plotting = pd.DataFrame(mob_data_for_plotting[['retail_pcfb',\n\u001b[0m\u001b[1;32m      3\u001b[0m               \u001b[0;34m'groc_pcfb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0;34m'parks_pcfb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0;34m'transit_pcfb'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/standard_38/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2906\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2908\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2910\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/standard_38/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/standard_38/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['res_pcfb'] not in index\""
     ]
    }
   ],
   "source": [
    "# count the number of nan values in the mobility data\n",
    "count_nan_mob_data_for_plotting = pd.DataFrame(mob_data_for_plotting[['retail_pcfb',\n",
    "              'groc_pcfb',\n",
    "              'parks_pcfb',\n",
    "              'transit_pcfb',\n",
    "              'work_pcfb',\n",
    "              'res_pcfb']\n",
    "             ].isnull().groupby(mob_data_for_plotting['sub_region_1']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-surfing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
